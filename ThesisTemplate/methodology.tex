\section{Methodology}
\label{sec:meth}


\subsection{Description of the data}
% Data verzameling en beschrijving van de data.
% Hoe is de data verzameld, en hoe heb jij die data verkregen?
% Wat staat er in de data? Niet alleen maar een technisch verhaal, maar ook inhoudelijk. DE lezer moet een goed idee krijgen over de technische inhoud en wat het betekent.

The data for the research project is provided by the subsidiary Work Participation and Income (WPI) of the municipality of Amsterdam. The data was extracted with SQL queries from three different databases. In total a corpus of 22 files is extracted, 17 CSV files and 5 XLSX files. A full description of all features in the files can be found in appendix XXXX. At a high level the data can be segmented in three categories: 1) information related to the welfare beneficiaries, 2) information linked to the job vacancies, and 3) information about the outcomes of welfare beneficiary and job vacancy matches. Each category will be elaborated.

The source of the welfare beneficiary information is a system called RAAK (this abbreviation translates to English as “on target”). RAAK is the primary source that is leveraged to model a representation of a welfare beneficiary (or user). When a person applies for welfare benefits there will be an intake with a so-called customer manager. The core task of the customer manager is to get the welfare beneficiary employed. During the intake personal data related to the employability of the welfare beneficiary is entered into the RAAK system. From this system a historical dataset spanning five years (2014-2019) is extracted by applying SQL queries. The output is 17 separate files in the CSV (Comma-Separated Values) format. The data stored in de RAAK database is structured and can be categorized in three types: 1) time data (DateTime), 2) numerical data (Float), and 3) categorical data (String). The size per data file varies between 23,529 and 663,403 rows.  
The data stored in the RAAK database originates from different sources, with most data generated by users of the system. Other data can be traced back to third parties sources. An example is the information regarding education history. This information is supplied in three different CSV files. The first file contains the education history that was registered in RAAK during the intake by the customer manager. The second file consists of education data retrieved from DUO, which is the Dutch government agency responsible for education. And finally the third file encompasses education data acquired from the UWV, which is the Dutch government agency responsible for delivering employee benefits to citizens, (i.e. the payout of unemployment benefits). The information in the previously mentioned files can overlap or be complementary to each other. During the data pre-processing phase it is necessary to transform and extract the information stored in these three files into one attribute per welfare beneficiary. 

Picture data distribution etc et


The second category is data related to the job vacancies (or items). The before mentioned Dutch governmental agency UVW also manages a job vacancy database named WBS. The WBS database lists all job openings deemed to be relevant for receivers of welfare benefits. Employers can register job openings in the WBS system. The incentive for employers to register vacancies in this system is that the Dutch government grants financial perks to companies who employ welfare beneficiaries.
In contrary to most job vacancy sites, the data saved in WBS is an mix of structured and unstructured data. The structured data is made up of numerical and categorical data, while the unstructured data consists of free text (i.e. job and task descriptions). It is  required  to assess if the structured data provides enough information to build sufficient job features. The historical dataset dating back five years (2014-2019) containing approximately 12,776 records  will be retrieved from the WBS database. In contrary to most job vacancy sites the data regarding the job offerings will be a mix of structured and unstructured data. Examples of unstructured data fields are the job description and task description. The data extracted from the WBS database is exported to a secure virtual environment. The output files are in the Excel format.
Because multiple data files from the same source are retrieved, the files partly overlap each other. Furthermore, some files contain information that is superfluous for the research, like for example the contact details of customer manager who entered the information into the system. Therefore, after carefully studying the content of the files only one file containing the job vacancy details is considered to be useful for this research project.

Finally the third category is categorical data about the outcome of the proposed matches between a welfare beneficiary and a job opening. After a welfare beneficiary is presented the employer provides feedback of the outcome of the match. The evaluation is registered in a system of the UWV called GIP. From the GIP database the matching results are retrieved and outputted as an Excel file.
There are three possible outcomes of a match: 1) the welfare beneficiary is hired, 2) is not hired, and 3) did not show up. For the latter group it cannot be determined if there was a match or not. Therefore, these observations should be removed during the pre-processing of the data because they are a possible source of noise. The data file retrieved contains 60,355 records and for 27,695 records the match outcomes are documented. 







% \pagebreak
% \subsection{Wat plotjes en tabelletjes}

% Zie het IPython Notebook \url{PandasAndLatex.ipynb} voor de code om vanuit pandas een poltje op te slaan en een dataframe als tabel op te slaan. Het werkt ideaal! 

% De interrupties van Wilders staan beschreven in Figure~\ref{fig:wilders} en Tabel~~\ref{tab:Wilders}.


% \begin{figure}
% \begin{center}
% \includegraphics[width=\linewidth]{WildersPlot.png}
% \caption{\label{fig:wilders} Aantal interrupties van Wilders in de Tweede Kamer door de tijd (periode 2012-2016).}
% \end{center}
% \end{figure}


% \pagebreak

% \begin{table}[h]
% \begin{footnotesize}
% \input{WildersTable}
% \end{footnotesize}
% \caption{\label{tab:Wilders} Door wie werd Wilders onderbroken en hoe vaak per debat.}
% \end{table}



\subsection{Data Science Pipeline & Methods}
Hoe je je vraag gaat beantwoorden.
% Dit is de langste sectie van je scriptie. 
% Als iets erg technisch wordt kan je een deel naar de Appendix verplaatsen. 
% Probeer er een lopend verhaal van te maken.
% Het is heel handig dit ook weer op te delen nav je deelvragen:

% \subsubsection{RQ1}

% \subsubsection{RQ2}

\subsubsection{Clustering}
The hypothesis is that there are different strata within the welfare beneficiaries and job vacancies. 
The welfare beneficiaries can for example be classified based on education level, age, work experience and be placed in distinct strata.
Furthermore it can be argued that an individual group of welfare beneficiaries can be linked to a particular set of job vacancies which are best matching. 
If it can be proven that it is plausible to realize stratification this can simplify the recommendation objective. 
A welfare beneficiary is, based on its features, allocated to a strata of welfare beneficiaries and connected to one or more sets of job vacancies. 
The objective here is to predict to which strata a welfare beneficiary belongs to that is linked to a particular set of job vacancies. 

The welfare beneficiary and job vacancy strata are unlabeled thus there are only two ways to stratify the data: 1) manual annotation based on domain knowledge, or 2) apply a unsupervised clustering algorithm.
The first way is labor intensive and time consuming due to the number of records, and because of time constraints is this method not feasible. 
For the second way there are multiple algorithm choices possible, however the data is mainly categorical with only a few numerical columns what limits the possibilities. 
Two unsupervised clustering algorithms are investigated: K-Prototypes which can handle a combination of numerical and categorical data \cite{huang1997clustering}, and K-Modes which deals only with categorical data \cite{huang1997clustering, huang1998extensions}.
The find the best cluster size (K) a limited but ascending number of K’s are explored, and evaluated by plotting the cost function (Elbow plot) and by adding the clusters to features and determine if the prediction function improves.

\subsubsection{Content-Based Recommender}
For the content-based recommender six different types of algorithms will be explored: 1) Nearest Neighbors, 2) K-Nearest Neighbors Classification, 3) Random Forest Classifier, 4) Binary Logistic Regression, 5) Linear Support Vector Classification, and 6) Neural Network. 
The first algorithm that is implemented is Nearest Neighbors, this algorithm is relatively straightforward and the outcomes are easy to assess 
\cite{aggarwal2016recommender}. 
The results of Nearest Neighbors serves as a benchmark to which the other algorithms are compared to. 
An extension of Nearest Neighbors is K-Nearest Neighbors Classification which is an algorithm that computes the class label from a majority vote of the K nearest neighbors \cite{NearestNeighbors}. 
An important difference between the two is that Nearest Neighbors is an unsupervised algorithm, while K-Nearest Neighbors Classification is supervised because during the training a set of correct class labels is provided.
The K-Nearest Neighbors Classification algorithm can therefore predict binary labels.
The Random Forest Classifier is a ruled based algorithm that is constructed out of multiple decision trees on randomly selected data samples \cite{breiman2001random}.
It predicts a class label by getting the predictions of each tree and selecting the best label by means of voting.
The algorithm learns rules that determine a class label, or in other words it learns which combination of features constitutes to a particular class label.

As fourth a regression-based model is implemented.
This type of model is explored because it learns a linear combination of features to predict a certain outcome. 
The learning objective is to predict a binary label (0 : no match, 1: match). 
From literature it can be derived that the binary logistic regression is particularly suitable for this problem \cite{aggarwal2016recommender}. 
Another often applied algorithm in case of binary ratings are support vector machines (SVM) \cite{burges1998tutorial}. 
This approach is similar to logistic regression with as main difference that SVM’s use a hinge loss rather then a logit function.
The advantage of the hinge loss is that it enables the SVM to learn non-linear patrons in the data. 
There is a myriad of subtypes within the family of SVM’s. For  this research the implementation of a linear support vector classification (SVC) is investigated because it is applicable for binary classification problems \cite{fan2008liblinear}. 

Singhal et al (2017) surveyed the use of Neural Networks for recommender systems \cite{singhal2017use}, and their research shows that Neural Networks are currently used in both collaborative filtering and in content-based filtering. 
The implementation of a Neural Network is researched because it excels in learning complex non-linear relationships between variables, and due to this trait it can learn patterns that humans are unable to identify. 
