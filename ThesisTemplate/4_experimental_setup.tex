\section{Experimental Setup}
\label{sec:setup}
%introduction
This section describes the particularities of the conducted research regarding the data (\ref{ssec:data}), the hyperparameter tuning of the tested models (\ref{ssec:ht}), and the evaluation procedure (\ref{ssec:eval}). 
The tooling used for this research project is Python and its relevant packages.

%Data
\subsection{Data}
\label{ssec:data}
The data for this research is provided by the municipality of Amsterdam and the Dutch social security agency (UWV).
The \textit{users} are welfare beneficiaries living in the city of Amsterdam, the \textit{items} are the job vacancies and the \textit{labels} are the outcomes of the job matching process.

% \noindent
% \textbf{Sources of the Data}\\
The user data is stored in a customer management system database called RAAK which is operated by the municipality.
Due to the abundance of user attributes in combination with resource constraints a preselection is required. 
Based on previous research done by the municipality and in collaboration with the domain experts the attributes to be extracted from this database are determined. 
The job vacancy data is retrieved from a database maintained by the Werkgeversservicepunt greater Amsterdam (WSP).
The UWV hosts a database named GIP in which the employer feedback regarding a job candidate is registered.
Therefore the labels can be derived from a dataset retrieved from the GIP database.

% \noindent
% \textbf{Choosing to Right Timespan}\\
According to consulted domain experts of the municipality of Utrecht the dynamics of the labor market makes it necessary to review job candidate hiring criteria at least every six months.
The economic crisis and its aftermath for example resulted in an oversupply on the labor market causing the hiring criteria of employers to rise making it harder for welfare beneficiaries to find a job.
In the current economic upturn and undersupply of labor, the employer hiring criteria are lowered month after month. 
When the labor market dynamics are taken into account a suitable time span would therefore be six months.
However, taking a six month time span would result in a dataset that is too small to learn patterns from.
Another aspect influencing the time span is the data quality.
The procedures by which user (welfare beneficiary) and job vacancy information is gathered and stored change over time.
As a result, there are many irregularities between recent data and data from several years ago.
Therefore a time span of five years was chosen, which strikes a balance between the maximum of available data, its quality, and the labor market dynamics.

% \noindent
% \textbf{Creating One Dataset}\\
In total 19 datafiles are provided for this research, 17 coming from the RAAK database and the other two coming from the WSP and GIP database.
The GIP data contains the dates on which the job matches occurred. 
Because the representation of a user can change over time, the data is merged together based on the match date. 
The resulting dataset consists of 12,482 observations.
However, not all job matching outcomes are documented properly and when the unlabeled (or weak labeled) observations are filtered out 8,265 records remain.

% \noindent
% \textbf{Feature Engineering}\\
The datasets contains dominantly categorical data, with just a few numerical and free text columns.
A number of user attributes carry sensitive personal information and these attributes are removed from the dataset or masked in accordance with the K-anonymity \cite{sweeney2002k} and the privacy by design \cite{d2015privacy} principles.
The masking of the data involves the transformation of numerical data into binary representation by first putting it in bins and then one-hot encoding it. 
All categorical data is also one-hot encoded for two reasons. 
Firstly, to deal with any missing values, and secondly to make the data digestible for all proposed machine learning algorithms. 
For all fields containing free text the punctuation and stopwords are removed, and the remaining words are stemmed, this handling is also known as Natural Language Processing (NLP).

% \noindent
% \textbf{The Final Dataset}\\
The final dataset consists of 8,265 observations, made up out of 6,216 unique users and 4,983 unique job vacancies. 
The labels are distributed as follows: 6,490 (79\%) positive and 1,775 (21\%) negative job matching outcomes have been documented. 
The 438 feature columns of the dataset are composed of 34 user features spread over 282 columns and 22 job features spread over 156 columns. 

%Hyperparameter Tuning
\subsection{Hyperparameter Tuning}
\label{ssec:ht}
With an iterative process (or hyperparameter search) the best hyperparameters for the machine learning models are determined. 
For the implementation of the machine learning models the Python library Scikit-learn\footnote{\url{https://scikit-learn.org/}\label{note1}} was used.
Only the most important choices will be highlighted here, a full listing of all hyperparameters that deviate from the default settings in Scikit-learn can be found in appendix \ref{ssec:ushp}.

For clustering the optimal number of clusters is determined by plotting the values of the cost function against the K’s, a method that is also called an Elbow plot.
In general the point where the curve starts to decrease and flatten out is the optimum number of K. 
However, determining this point can be rather subjective and deciding on the number of K is often combined with domain knowledge.
For the hierarchical clustering of the n-grams of job title characters the ideal n-gram length is determined iteratively. 

The optimum number of neighbors for K-Nearest Neighbors is determined iteratively by testing all K’s in the range of 2 till 50 and plotting the scores in a graph (appendix \ref{ssec:fonn}).
The best hyperparameters for RandomForest are found with the use of Scikit-learn’s\textsuperscript{\ref{note1}} randomized search with cross validation for hyperparameters module \cite{ bergstra2012random}.
For Logistic Regression the hyperparameters are tailored towards the binary classification problem. 
None of the default hyperparameters are altered for Linear Support Vector Classification. 

The search for the best hyperparameters for the Neural Network is more comprehensive and there is no standard setting for a particular problem. 
The number of settings for a Neural Network is unbound, and while there are many strategies devised for finding the best hyperparameters for neural networks \cite{larochelle2009exploring} it remains a trial and error process.
The strategy deployed for this research is to start off with a setting with one hidden layer and first determine the best activation function and solver amongst the other settings.
Hereafter, the optimum number of layers and nodes in a layer are explored iteratively.

%Evaluation Methods
\subsection{Evaluation Methods}
\label{ssec:eval}
In this part the methods to evaluate the training results of the proposed models are outlined.
The evaluation methods can be divided into three types: 1) the evaluation of the clustering, 2) the evaluation of the classification models, and 3) the evaluation of the rankings.

Clustering is an unsupervised algorithm and therefore validation against withheld examples is not possible.
However, the clustering results can be evaluated by adding them as features or labels to the dataset.
In case of adding them as features it can be tested if the prediction results improve when the clusters are added.
Multiple setups can be used for this, like for example using the user features in combination with the job clusters to predict the labels.
Another method is to try to predict the clusters on the basis of the features, for example using the user features to predict the job clusters. 

For training and validation of all classification models a split of 80/20 based on saved indices is used. 
The saved indices were created with the help of a group shuffle\textsuperscript{\ref{note1}} with taking into account that the same users are not allowed to be present in both the training and validation set.
The train/validation split is shown in table \ref{tab:tvs}.
For the evaluation of the classification models the Matthews Correlation Coefficient (MCC) \cite{matthews1975comparison} and accuracy are used as metrics.
The MCC is a metric to measure the quality of binary classifications, and according to Chicco (2017) this metric is useful because it is the only metric that takes into account the ratio of confusion matrix size \cite{chicco2017ten}.
In case of imbalanced datasets the MCC is able to indicate correctly if the prediction evaluation is preceding well or not, while metrics as accuracy and F-measure are not \cite{boughorbel2017optimal}.

\begin{table}[h]
\begin{footnotesize}
\input{ThesisTemplate/Tables/TableTrainValSplit.tex}
\end{footnotesize}
\caption{\label{tab:tvs} \footnotesize{Train/Validation Split}}
\end{table}

The evaluation of the rankings can be done in various ways, although for this research a two step method is used. 
The first step is to calculate the average rank of the labeled observations in the user $\times$ jobs matrix.
If the calculated average rank deviates from the mean total rank (or the midpoint of all ranks) towards the lower end it can be assumed that the predictions are better than random selection. 
Because the true correctness of a ranking is hard to capture in metrics the second step proposes a user study, in which the sets of \textit{n}-top rankings are evaluated by domain experts. 